\documentclass{article}

\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath,siunitx}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{multirow}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{changepage}

\usetikzlibrary{automata,positioning}

\usepackage{biblatex} 
\addbibresource{bibliography.bib} % Import the bibliography file

% 
% Renewal of commands
%
\makeatletter
\renewenvironment{cases}[1][l]{\matrix@check\cases\env@cases{#1}}{\endarray\right.}
\def\env@cases#1{%
  \let\@ifnextchar\new@ifnextchar
  \left\lbrace\def\arraystretch{1.2}%
  \array{@{}#1@{\quad}l@{}}}
\makeatother

%
% Basic Document Settings
%

\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.0in
\headsep=0.25in

\linespread{1.1}

\pagestyle{fancy}
\lhead{\hmwkTeam}
\chead{\hmwkClass: \hmwkTitle}
\rhead{\firstxmark}
\lfoot{\lastxmark}
\cfoot{\thepage}

\renewcommand\headrulewidth{0.4pt}
\renewcommand\footrulewidth{0.4pt}

\setlength\parindent{0pt}

\newcommand{\setsep}{,    \ }

%
% Create Problem Sections
%

\newcommand{\enterProblemHeader}[1]{
    \nobreak\extramarks{}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
}

\newcommand{\exitProblemHeader}[1]{
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1} (continued)}{Problem \hmwkNumber.\arabic{#1} continued on next page\ldots}\nobreak{}
    \stepcounter{#1}
    \nobreak\extramarks{Problem \hmwkNumber.\arabic{#1}}{}\nobreak{}
}

\setcounter{secnumdepth}{0}
\newcounter{partCounter}
\newcounter{homeworkProblemCounter}
\setcounter{homeworkProblemCounter}{1}
\nobreak\extramarks{Problem \arabic{homeworkProblemCounter}}{}\nobreak{}

%
% Homework Problem Environment
%
% This environment takes an optional argument. When given, it will adjust the
% problem counter. This is useful for when the problems given for your
% assignment aren't sequential. See the last 3 problems of this template for an
% example.
%
\newenvironment{homeworkProblem}[2][-2]{
    \ifnum#1>0
        \setcounter{homeworkProblemCounter}{#1}
    \fi
    \section{Problem \hmwkNumber.\arabic{homeworkProblemCounter} #2}
    \setcounter{partCounter}{1}
    \enterProblemHeader{homeworkProblemCounter}
}{
    \exitProblemHeader{homeworkProblemCounter}
}

%
% Homework Details
%   - Title
%   - Due date
%   - Class
%   - Section/Time
%   - Instructor
%   - Author
%
\newcommand{\hmwkNumber}{H9}
\newcommand{\hmwkTitle}{Homework Assignment \hmwkNumber}
\newcommand{\hmwkClass}{DIC}
\newcommand{\hmwkTeam}{Team \#11}
\newcommand{\hmwkAuthorName}{\hmwkTeam: \\ Camilo Mart√≠nez 7057573, cama00005@stud.uni-saarland.de \\ Honglu Ma 7055053, homa00001@stud.uni-saarland.de \\ Maria Sibi 7058469, mama00047@stud.uni-saarland.de}

%
% Title Page
%

\title{
    % \vspace{2in}
    \textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
}

\author{\hmwkAuthorName}
\date \today

\renewcommand{\part}[1]{\textbf{\large Part \Alph{partCounter}}\stepcounter{partCounter}\\}

%
% Various Helper Commands
%

% Useful for algorithms
\newcommand{\alg}[1]{\textsc{\bfseries \footnotesize #1}}

% For derivatives
\newcommand{\deriv}[1]{\frac{\mathrm{d}}{\mathrm{d}x} (#1)}

% For partial derivatives
\newcommand{\pderiv}[2]{\frac{\partial}{\partial #1} (#2)}

% Integral dx
\newcommand{\dx}{\mathrm{d}x}

% Define argmax and argmin
\DeclareMathOperator*{\argmax}{argmax}
\DeclareMathOperator*{\argmin}{argmin}

% Alias for the Solution section header
\newcommand{\solution}{\textbf{\large Solution}}

% Probability commands: Expectation, Variance, Covariance, Bias
\newcommand{\E}{\mathrm{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\Bias}{\mathrm{Bias}}

\begin{document}

\maketitle

\begin{homeworkProblem}{(Joint Image- and Flow-driven Regularisation)}
    \subsection*{(a)}
    \vspace*{-1.9em}
    \begin{adjustwidth}{2.5em}{0pt}

    We can expand $S$ given the fact $\partial_vu := v^\top\nabla u$
    \begin{align*}
        S   &=  |\partial_vu_1|^2 + |\partial_vu_2|^2\\
            &=  |v^\top\nabla u_1|^2 + |v^\top\nabla u_2|^2\\
            &=  v^\top\nabla u_1v^\top\nabla u_1 + v^\top\nabla u_2v^\top\nabla u_2\\
            &= ({u_1}_{x_1}v_1+{u_1}_{x_2}v_2)^2 + ({u_2}_{x_1}v_1+{u_2}_{x_2}v_2)^2
    \end{align*}
    We calculate ${S}_{{u_1}_{x_1}}$ and ${S}_{{u_1}_{x_2}}$
    \begin{align*}
        {S}_{{u_1}_{x_1}}   &=  2v_1({u_1}_{x_1}v_1+{u_1}_{x_2}v_2)\\
                            &=  2v_1(v^\top\nabla u_1)\\
        {S}_{{u_1}_{x_2}}   &=  2v_2({u_1}_{x_1}v_1+{u_1}_{x_2}v_2)\\
                            &=  2v_2(v^\top\nabla u_1)
    \end{align*}
    We have the gradient descent equation
    \begin{align*}
        \partial_t u_1  &=  -\gamma\nabla_{u_1}E\\
                        &=  -\gamma(F_{u_1} - \partial_{x_1}F_{{u_1}_{x_1}} - \partial_{x_2}F_{{u_1}_{x_2}})\\
                        &=  \gamma(\partial_{x_1}F_{{u_1}_{x_1}} + \partial_{x_2}F_{{u_1}_{x_2}} - F_{u_1})\\
        \intertext{where $F := M(u, f) + \alpha S(\nabla u)$ so}
        \partial_t u_1  &=  \gamma(\partial_{x_1}{S}_{{u_1}_{x_1}}  + \partial_{x_2}{S}_{{u_1}_{x_2}} - \partial_{u_1}M)\\
                        &=  \gamma(2v_1(v^\top\nabla u_1)  + 2v_2(v^\top\nabla u_1) - \partial_{u_1}M)\\
                        &=  \gamma(\partial_{x_1}(2v_1(v^\top\nabla u_1))  + \partial_{x_2}(2v_2(v^\top\nabla u_1)) - \partial_{u_1}M)\\
                        &=  \gamma(2 \, \mathrm{div}(vv^\top\nabla u_1) - \partial_{u_1}M)
    \end{align*}
    One can observe that $vv^\top$ is a diffusion tensor. The derivation is similar for $\partial_t u_2$.
    \end{adjustwidth}
    
    \subsection*{(b)}
    \vspace*{-1.9em}
    \begin{adjustwidth}{2.5em}{0pt}
    The derivation of the gradient descent equation is similar to the one above.
    The changes are the ${S}_{{u_1}_{x_1}}$ and ${S}_{{u_1}_{x_2}}$ terms in $\partial_t u_1$
    because now we have $S = \Psi(|\partial_vu_1|^2 + |\partial_vu_2|^2)$
    \begin{align*}
        {S}_{{u_1}_{x_1}}   &=  2\Psi'(|\partial_vu_1|^2 + |\partial_vu_2|^2)v_1({u_1}_{x_1}v_1+{u_1}_{x_2}v_2)\\
                            &=  2\Psi'(|\partial_vu_1|^2 + |\partial_vu_2|^2)v_1(v^\top\nabla u_1)\\
        {S}_{{u_1}_{x_2}}   &=  2\Psi'(|\partial_vu_1|^2 + |\partial_vu_2|^2)v_2({u_1}_{x_1}v_1+{u_1}_{x_2}v_2)\\
                            &=  2\Psi'(|\partial_vu_1|^2 + |\partial_vu_2|^2)v_2(v^\top\nabla u_1)    
    \end{align*}
    Thus
    \[
        \partial_t u_1 = \gamma(2\mathrm{div}(\Psi'(|\partial_vu_1|^2 + |\partial_vu_2|^2)vv^\top\nabla u_1) - \partial_{u_1}M)
    \]
    \end{adjustwidth}

\end{homeworkProblem}

\begin{homeworkProblem}{(Algorithms for the Parabolic and Elliptic Problem)}
    For the following energy functional:

    \[
    E(u, v) = \frac{1}{2} \int_{\Omega} \left( \bm{w}^\top \bm{J} \bm{w} + \alpha (\lvert \nabla u\ \rvert ^2 + \lvert \nabla v \rvert ^2) \right) dx dy
    \]

    where $\bm{w} = (u, v, 1)^\top$ denotes the optic flow, and $\bm{J} = (J_{i,k}) : \Omega \rightarrow \mathbb{R}^{3 \times 3}$ is some motion tensor field, its minimiser satisfies the Euler--Lagrange equations:
    \[
    \begin{aligned}
    \Delta u_i - \frac{1}{\alpha} (J_{1,1,i} u_i + J_{1,2,i} v_i + J_{1,3,i}) &= 0, \\
    \Delta v_i - \frac{1}{\alpha} (J_{2,1,i} u_i + J_{2,2,i} v_i + J_{2,3,i}) &= 0,
    \end{aligned}
    \]

    where \( J_{n,m,i} \) is the component \( (n, m) \) of the structure tensor $\bm{J}$ for some pixel $i$ for $i = 1,\dots, N$. This constitutes a linear system of equations for the $2N$ unknowns $(u_i)$ and $(v_i)$.

    \subsection*{(a)} 
    \vspace*{-1.9em}
    \begin{adjustwidth}{2.5em}{0pt}

    We discretize with a \textit{modified explicit scheme} with an \textit{implicitly stabilised} data term:
    \begin{align*}
    \frac{u_i^{k+1} - u_i^k}{\tau} &= \Delta u_i^k - \frac{1}{\alpha} \left( J_{1,1,i} u_i^{k+1} + J_{1,2,i} v_i^{k} + J_{1,3,i} \right), \\
    \frac{v_i^{k+1} - v_i^k}{\tau} &= \Delta v_i^k - \frac{1}{\alpha} \left( J_{2,1,i} u_i^{k} + J_{2,2,i} v_i^{k+1} + J_{2,3,i} \right)
    \end{align*}

    where $\Delta u_i^k$ and $\Delta v_i^k$ are defined at iteration $k$ and pixel $i$ as follows 
    \begin{align*}
        \Delta u_i^k &= \sum_{j \in N(i)} \frac{u_j^k - u_i^k}{h^2}, \\
        \Delta v_i^k &= \sum_{j \in N(i)} \frac{v_j^k - v_i^k}{h^2}
    \end{align*}

    Thus, the Gauss--Seidel method for the Parabolic problem can be written as
    \begin{align*}
        u_i^{k+1} &= \frac{\tau \left[ \Delta u_i^k - \frac{1}{\alpha} \left( J_{1,2,i} v_i^{k} + J_{1,3,i} \right)\right] + u_i^k}{1 + \frac{\tau}{\alpha} J_{1,1,i}}, \\
        v_i^{k+1} &= \frac{\tau \left[ \Delta v_i^k - \frac{1}{\alpha} \left( J_{2,1,i} u_i^{k} + J_{2,3,i} \right)\right] + v_i^k}{1 + \frac{\tau}{\alpha} J_{2,2,i}}
    \end{align*}
    \end{adjustwidth}

    \subsection*{(b)} 
    \vspace*{-1.9em}
    \begin{adjustwidth}{2.5em}{0pt}

    Following Lecture 18 and taking into account that the Jacobi method is the same as the Gauss--Seidel method, except that in the latter one uses at each stage the values $u_i^{k+1}$ when available \cite*{book:IterativeSolutionLargeLinearSystems}, the Jacobi iterative scheme for the Elliptic problem is given by
    \begin{align*}
        u_i^{k+1} &= \frac{\sum\limits_{j \in N(i)} u_j^{k} - \frac{h^2}{\alpha} \left( J_{1,2,i} v_i^k + J_{1,3,i} \right)}{|N(i)| + \frac{h^2}{\alpha} J_{1,1,i}}, \\
        v_i^{k+1} &= \frac{\sum\limits_{j \in N(i)} v_j^{k} - \frac{h^2}{\alpha} \left( J_{2,1,i} u_i^k + J_{2,3,i} \right)}{|N(i)| + \frac{h^2}{\alpha} J_{2,2,i}}
    \end{align*}
    where \( N(i) \) are the pixel indices of the neighbours of pixel $i$ and consecuently $\lvert N(i) \rvert$ denotes the number of neighbours of pixel $i$ that belong to the image domain. Note that, in contrast to the derivation shown in Lecture 18, we do not use either \( N^-(i) := \{j \in N(i) \, | \, j < i\} \) or \( N^+(i) := \{j \in N(i) \, | \, j > i\} \), since, in the Jacobi method, all the \( u_j^{k+1} \) and \( v_j^{k+1} \) in the sums are replaced with \( u_j^{k} \) and \( v_j^{k} \) respectively.

    \end{adjustwidth}

    \subsection*{(c)} 
    \vspace*{-1.9em}
    \begin{adjustwidth}{2.5em}{0pt}
    The Gauss--Seidel method should converge faster, since we are using recent values of the current $k+1$ iteration, that have already been calculated for the neighbours $j$ of the current pixel $i$. This also means that the order we choose to traverse the vector $u$ and $v$ matters, and each step depends on the previous one being completed, making it not parallelizable. In contrast, in the Jacobi method, the computations for $u_i^{k+1}$ and $v_i^{k+1}$ are decoupled within an iteration, that is, all new $k+1$ values are computed independently of each other based on the previous $k$-th iteration's values. 
    \end{adjustwidth}

\end{homeworkProblem}

\printbibliography

\end{document}
